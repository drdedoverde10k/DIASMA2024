{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Configuration (English)\n",
    "\n",
    "Download de dataset from: https://console.cloud.google.com/storage/browser/quickdraw_dataset/full/raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory\n",
    "output_dir = './quickdraw_data/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Descargar las clases seleccionadas\n",
    "for class_name in classes:\n",
    "    url = f'https://storage.googleapis.com/quickdraw_dataset/full/raw/{class_name}.ndjson'\n",
    "    response = requests.get(url)\n",
    "    file_path = os.path.join(output_dir, f'{class_name}.ndjson')\n",
    "    with open(file_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f'Download: {class_name}')\n",
    "\n",
    "# Yor Class are:\n",
    "# ['hammer', 'hot air balloon', 'microwave', 'snake', 'megaphone']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image convertion from njson to npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# image ndjson to numpy function\n",
    "def image_ndjson2numpy(data, img_size=51, size_line=10, only_recognized=True):\n",
    "    flagRecog = lambda x: x if only_recognized else lambda x: True\n",
    "    imgSize = (img_size, img_size) if isinstance(img_size, int) else img_size\n",
    "    images = []\n",
    "    labels = []\n",
    "    for xx in data:\n",
    "        if flagRecog(xx['recognized']):\n",
    "            try:\n",
    "                yy = np.array(xx['drawing'], dtype=np.object_)[:,:2].T\n",
    "                FF = [np.concatenate(yy[:,i], dtype=float).astype(int).reshape(2,-1).T[:,None] for i in range(len(xx['drawing']))]\n",
    "                minmaxFF = np.concatenate([np.c_[i.min(0), i.max(0)] for i in FF])\n",
    "                minFF = minmaxFF[:,:2].min(0, keepdims=True)\n",
    "                maxFF = minmaxFF[:,2:].max(0, keepdims=True)\n",
    "                FF = [i - minFF[None] for i in FF]\n",
    "                img = np.zeros(((maxFF - minFF).tolist()[0][1], (maxFF - minFF).tolist()[0][0]), np.uint8)\n",
    "                images.append(cv2.resize(cv2.polylines(img, FF, False, 1, size_line), imgSize))\n",
    "                labels.append(xx['class'])\n",
    "            except:\n",
    "                continue\n",
    "    return np.array(images), labels\n",
    "\n",
    "# Process images and labels\n",
    "images, labels = image_ndjson2numpy(data, img_size=51)\n",
    "\n",
    "# Combine images and labels in one numpy dataset\n",
    "np.save('data_all.npy', np_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño en memoria: 0.01 GB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_numpy = np.load('data_all.npy', allow_pickle=True)\n",
    "print(f\"Tamaño en memoria: {data_numpy.nbytes / 1e9:.2f} GB\")\n",
    "\n",
    "# Separar imágenes y etiquetas después del muestreo\n",
    "data_sample_images = np.array([item[0] for item in data_numpy])\n",
    "data_sample_labels = np.array([item[1] for item in data_numpy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Image number\n",
    "n = random.randrange(0, len(data_numpy) , 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(data_sample_images[n])  # We use cmap='gray' for gray scale images\n",
    "plt.title(f\"Your labels is: {data_sample_labels[n]}\")\n",
    "plt.axis() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extraer las etiquetas\n",
    "labels = np.array([item[1] for item in data_numpy])\n",
    "\n",
    "# Contar la frecuencia de cada etiqueta\n",
    "unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "# Graficar la distribución de clases\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(unique_labels, counts, color='skyblue')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Example Numbers')\n",
    "plt.title('data_numpy class distribution')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide datates in train and test (80/20), without stratify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Divide Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_sample_images, data_sample_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "# Transformation to tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Dataset personalizado para PyTorch\n",
    "class QuickDrawDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images*255  # Nueva escala ajustada\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "        # Crear el mapeo de clases a índices solo una vez\n",
    "        self.classes = list(set(labels))  # Todas las clases únicas\n",
    "        self.class_to_label = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx] # Número total de imágenes en el dataset\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        # Convertir la etiqueta en el índice correspondiente\n",
    "        label = self.class_to_label[label]\n",
    "        label = torch.tensor(label, dtype=torch.long)  # Aseguramos que sea un tensor de tipo entero\n",
    "        return img, label\n",
    "    \n",
    "\n",
    "train_dataset = QuickDrawDataset(X_train, y_train, transform=transform)\n",
    "val_dataset = QuickDrawDataset(X_test, y_test, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=5000, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=5000, shuffle=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Neural Network (Best Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Best CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, filter_size, stride, padding, fc1_size, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Parámetros iniciales\n",
    "        print(f\"Process begin: CNN de {filter_size} kernels, stride {stride}, padding {padding} y FC size {fc1_size}\")\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=filter_size, stride=stride, padding=padding)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=filter_size, stride=stride)\n",
    "        \n",
    "        # Calculate size before convolution and pooling\n",
    "        input_size = 51  # Initial size 51x51\n",
    "        conv_output_size = math.floor((input_size + 2 * padding - filter_size) / stride + 1)\n",
    "        pool_output_size = math.floor((conv_output_size - filter_size) / stride + 1)\n",
    "\n",
    "        print(f\"{conv_output_size} = {(input_size + 2 * padding - filter_size) / stride + 1}\")\n",
    "        print(f\"{pool_output_size} = {(conv_output_size - filter_size) / stride + 1}\")\n",
    "        \n",
    "        # FC Size\n",
    "        flattened_size = 32 * pool_output_size * pool_output_size\n",
    "        print(f\"Size before convolution and pooling: {flattened_size}\")\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(flattened_size, fc1_size)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(fc1_size, num_classes)\n",
    "        \n",
    "        # Activaciones\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs and Outputs of the train_model Function\n",
    "\n",
    "Inputs:\n",
    "1. model:\n",
    "- **Type**: torch.nn.Module\n",
    "- **Description**: The neural network model that will be trained and validated.\n",
    "\t2.\ttrain_loader:\n",
    "- **Type**: torch.utils.data.DataLoader\n",
    "- **Description**: DataLoader containing the training data organized in batches.\n",
    "\t3.\tval_loader:\n",
    "- **Type**: torch.utils.data.DataLoader\n",
    "- **Description**: DataLoader containing the validation data organized in batches.\n",
    "\t4.\tcriterion:\n",
    "- **Type**: torch.nn.Module (e.g., torch.nn.CrossEntropyLoss)\n",
    "- **Description**: Loss function that measures the difference between the model’s predictions and the actual labels.\n",
    "\t5.\toptimizer:\n",
    "- **Type**: torch.optim.Optimizer (e.g., torch.optim.SGD, torch.optim.Adam)\n",
    "- **Description**: Optimizer that adjusts the model weights based on the loss function.\n",
    "\t6.\tepochs (optional):\n",
    "- **Type**: int\n",
    "- **Description**: Number of iterations (epochs) for training the model. Defaults to 1000.\n",
    "\t7.\tearly_stopping_patience (optional):\n",
    "- **Type**: int\n",
    "- **Description**: Number of epochs without improvement in validation loss before early stopping is triggered. Defaults to 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEEV training cycle\n",
    "def train_cicle(model, train_loader, val_loader, criterion, optimizer, epochs=1000, early_stopping_patience=15):\n",
    "    device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "    print(f\"Usando el dispositivo: {device}\")\n",
    "\n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    # Metrics List\n",
    "    train_losses, val_losses, val_accuracies = [], [], []\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            # Secure teh right form of labels and images\n",
    "            if len(inputs.shape) == 3:  # If one chanel dimension missed\n",
    "                inputs = inputs.unsqueeze(1)\n",
    "            if len(labels.shape) > 1:  # If labels have one more dimension\n",
    "                labels = labels.squeeze()\n",
    "            \n",
    "            # move to device\n",
    "            inputs = inputs.to(device,dtype=torch.float32) # Images\n",
    "            labels = labels.to(device, dtype=torch.long) # Labels\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Forward and loss calculation\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Backward y optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Training loss media\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss) #/ len(train_loader))\n",
    "\n",
    "        # Evaluate \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        y_pred_prob = []  \n",
    "        y_true_binarized = []  \n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                \n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "                \n",
    "                # Predictions\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                y_pred.extend(predicted.cpu().numpy())  # Save predictions\n",
    "                y_true.extend(labels.cpu().numpy())\n",
    "                probas = F.softmax(outputs, dim=1).cpu().numpy()  # softmax\n",
    "                y_pred_prob.extend(probas)\n",
    "                y_true_binarized.extend(label_binarize(labels.cpu().numpy(), classes=range(len(classes))))\n",
    "                \n",
    "                \n",
    "                # saving\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        accuracy = 100 * correct / total if total > 0 else 0\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(accuracy)\n",
    "        y_pred_prob = np.array(y_pred_prob)\n",
    "        y_true_binarized = np.array(y_true_binarized)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}. Best Val Loss: {best_val_loss:.4f}\")\n",
    "                break\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {accuracy:.2f}%\")\n",
    "    \n",
    "    return train_losses, val_losses, val_accuracies,outputs, y_true, y_pred, y_true_binarized, y_pred_prob\n",
    "\n",
    "# Configuration\n",
    "filter_size = 5\n",
    "stride = 2\n",
    "padding = 1\n",
    "fc1_size = 64\n",
    "classes = list(set(data_sample_labels))\n",
    "num_classes = len(classes)\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Training\n",
    "epochs = 1000\n",
    "early_stopping_patience = 15\n",
    "model = CNN(filter_size, stride, padding, fc1_size, num_classes=num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.01) # Learning rate 0.01\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loss, val_loss, val_accur, outputs, y_true, y_pred, y_true_binarized, y_pred_prob  = train_cicle(\n",
    "    model, train_loader, val_loader, criterion, optimizer, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outputs:\n",
    "\n",
    "1. **train_losses:**\n",
    "    - **Type:** `list[float]`\n",
    "    - **Description:** A list containing the average training loss for each epoch.\n",
    "\n",
    "2. **val_losses:**\n",
    "    - **Type:** `list[float]`\n",
    "    - **Description:** A list containing the average validation loss for each epoch.\n",
    "\n",
    "3. **val_accuracies:**\n",
    "    - **Type:** `list[float]`\n",
    "    - **Description:** A list containing the accuracy on the validation set for each epoch.\n",
    "\n",
    "4. **accuracy:**\n",
    "    - **Type:** `float`\n",
    "    - **Description:** The final accuracy achieved on the validation set.\n",
    "\n",
    "5. **outputs:**\n",
    "    - **Type:** `torch.Tensor`\n",
    "    - **Description:** The model's final predictions (untransformed logits) generated during the validation phase.\n",
    "\n",
    "6. **y_pred:**\n",
    "    - **Type:** `list[int]`\n",
    "    - **Description:** A list containing the predicted labels accumulated during the validation epochs.\n",
    "\n",
    "7. **y_true:**\n",
    "    - **Type:** `list[int]`\n",
    "    - **Description:** A list containing the true labels corresponding to the validation samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP model with variable hidden size\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=2601, hidden_size=500, output_size=10):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        # if there hidden size\n",
    "        if hidden_size > 0:\n",
    "            self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "            \n",
    "            # Xavier init\n",
    "            nn.init.xavier_uniform_(self.fc1.weight)\n",
    "            nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        \n",
    "        else:  # No hidden size\n",
    "            self.fc1 = nn.Linear(input_size, output_size)\n",
    "            self.fc2 = None\n",
    "            \n",
    "            # Xavier Init\n",
    "            nn.init.xavier_uniform_(self.fc1.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        if self.fc2:  # With hidden layer\n",
    "            x = self.sigmoid(self.fc1(x))\n",
    "            x = self.fc2(x)\n",
    "        else:  # No hidden layer\n",
    "            x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# Network configuration\n",
    "input_size = 51 * 51\n",
    "hidden_size = 500 #[25, 100, 300, 1000, 0]  # N Values (0 = without hidden layer)\n",
    "epochs = 1000\n",
    "classes = list(set(data_sample_labels))\n",
    "num_classes = len(classes)\n",
    "learning_rate = 0.01"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
